{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MMWp3bQP-bhU"
   },
   "source": [
    "# Lab 5 - Classification : Naive Bayes and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n-5YUdzFsqwL"
   },
   "source": [
    "# Optical recognition of handwritten digits dataset\n",
    "**Download dataset from sklearn. The dataset has 10 classes and 64 attributes (8x8 images). Visualise images from the dataset. Perform a train test split in the ratio 4:1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6w_FSrzBE5pw"
   },
   "source": [
    "# Using sklearn (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ANyyLqRaE-RA"
   },
   "source": [
    "**For this exercise, you will use the naive bayes and logistic regression functions in sklearn. Use the optical recognition dataset.**\n",
    "\n",
    "\n",
    "**a) Logistic Regression - use one vs all classification method to classify the dataset into one of the ten classes. Report the accuracies in terms of F1 scores and the confusion matrix (use sklearn functions for this too). Tune parameters to obtain the best results.**\n",
    "\n",
    "**b) Naive Bayes - perform multiclass classification to classify the dataset into one of the ten classes. Experiment with all the priors available (Gaussian, Bernoulli, etc) and report the best prior. Report the accuracies in terms of F1 scores and the confusion matrix (use sklearn functions for this too).**\n",
    "\n",
    "**Estimated Time: 50 mins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EvL8quVdN24q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The images in the given dataset are as follows:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1MAAACVCAYAAAC94aCoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAALc0lEQVR4nO3dX2id93kH8OdpVNbRLJJysV5sEFmjG9sucohzN7adgExYYZNg2HTLhhUYNi0ZKOxCumhB7gq1YTD7outcyCKzjkEMnbR/MOLNCqwXYxaTLwpdRiJlHWtZu0hekrbZ6H67kDKaun+c55X6+sifDxywD3zP+1g+en/vV7+jc7K1FgAAALwz7+p7AAAAgFGkTAEAABQoUwAAAAXKFAAAQIEyBQAAUKBMAQAAFChTAAAABcpURGTmg5n5Z5n5Rma+kpm/3vdMjIbMfCozb2Tmm5m50vc8jI7M/KHMfGb/nPNaZv5TZv5S33MxGjLzM5n5pcz8r8x8MTN/q++ZGD2Z+f7M/EZmfqbvWRgdmbm+/7x5ff/2z33P1Cdlas8nI+K/I+J9EfFERHwqM3+235EYEf8eER+PiD/qexBGzlhEfDEifjEixiPioxHxXGZO9TgTo+MTETHVWnsgIn4lIj6emcd7nonR88mI+Me+h2AkPdVau3//9lN9D9One75MZeZ7I+JXI+KjrbXXW2t/HxF/HhG/2e9kjILW2mdba6sR8Z99z8Joaa290Vpbbq1tt9b+t7X2lxGxFREuiPm+Wmufb629+dZf928/0eNIjJjM/GBE7EbE3/Y9C4yye75MRcRPRsQ3W2svfst9NyPCzhTwA5OZ74u989Hn+56F0ZCZf5CZX4uIL0TElyLir3seiRGRmQ9ExMci4nf6noWR9YnM/Gpmfi4zh30P0ydlKuL+iLj1bffdiogf6WEW4B6Ume+OiD+JiCuttS/0PQ+jobX24dhbq34+Ij4bEW9+7wT8v9+NiGdaa1/sexBG0mJETEfEj0XEpyPiLzLznt0ZV6YiXo+IB77tvgci4rUeZgHuMZn5roj449j7vc2neh6HEdNa++b+y9N/PCI+1Pc83P0ycxARMxHx+33Pwmhqrf1Da+211tqbrbUrEfG5iPhA33P1ZazvAe4CL0bEWGa+v7X2L/v3PRxeagMcsszMiHgm9t785gOttf/peSRG11j4nSnuzDAipiLiX/dOQXF/RNyXmT/TWnukx7kYXS0isu8h+nLP70y11t6IvZdHfCwz35uZPxcRs7H3k2L4njJzLDPfExH3xd5i9J7M9EMK7tSnIuKnI+KXW2tf73sYRkNm/mhmfjAz78/M+zLz8Yj4tYj4u75nYyR8OvaK92D/9ocR8VcR8XifQzEaMnMiMx9/63onM5+IiF+IiL/pe7a+3PNlat+HI+KHI+I/IuJPI+JDrTU7U9yJj0TE1yNiKSJ+Y//PH+l1IkZCZj4UEWdj72Lmy9/yeR1P9Dwad78Wey/p+7eI2ImI34uIhdbaWq9TMRJaa19rrX35rVvs/brDN1prX+l7NkbCu2PvI2G+EhFfjYjfjoi51to9+1lT2VrrewYAAICRY2cKAACgQJkCAAAoUKYAAAAKlCkAAICC7/cWzr28O8XVq1fL2cXFxXL2xIkT5ez58+fL2cnJyXK2o8P+TICRe3eT4XBYzu7u7paz586dK2dnZ2fL2Y4O8/kzcs+d9fX1cnZubq6cHQwG5WyXmTs6kueeCxculLNLS0vl7LFjx8rZjY2NcvaIrl0jd+7psvbMz8+Xs6urq+Vsj47kuafLtcvU1FQ5u7KyUs6OqO/4/LEzBQAAUKBMAQAAFChTAAAABcoUAABAgTIFAABQoEwBAAAUKFMAAAAFyhQAAECBMgUAAFCgTAEAABQoUwAAAAXKFAAAQIEyBQAAUKBMAQAAFIz1PcB3sri4WM5ubW2Vszs7O+Xsgw8+WM4+99xz5ezJkyfLWW43MTFRzr7wwgvl7PXr18vZ2dnZcpa329zcLGcfe+yxcnZ8fLyc3d7eLme53dLSUjnb5Vx++fLlcvbs2bPl7MbGRjk7MzNTznJwVlZWytnBYHBwg9CbLutAl2uXK1eulLMPPfRQOXu3rXt2pgAAAAqUKQAAgAJlCgAAoECZAgAAKFCmAAAACpQpAACAAmUKAACgQJkCAAAoUKYAAAAKlCkAAIACZQoAAKBAmQIAAChQpgAAAAqUKQAAgIKxw3rgjY2NcnZra6ucfemll8rZ6enpcvbEiRPlbJev1cmTJ8vZo2pzc7OcXV9fP7hB3oHBYNDLcXm71dXVcvbhhx8uZ+fm5srZc+fOlbPc7syZM+Xs4uJiOXv8+PFy9tixY+XszMxMOcvB2d3dLWdXVlbK2YWFhXJ2e3u7nO1iamqql+PezSYmJsrZV155pZwdHx8vZ4fDYTnb5fuly9fqu7EzBQAAUKBMAQAAFChTAAAABcoUAABAgTIFAABQoEwBAAAUKFMAAAAFyhQAAECBMgUAAFCgTAEAABQoUwAAAAXKFAAAQIEyBQAAUKBMAQAAFIwd1gPv7OyUs4888kg5Oz09Xc52cfz48V6Oe1RdvHixnF1eXi5nb926Vc52MRwOezkub7ewsFDOTk1N9XLc2dnZcpbbdVlDXn755XJ2a2urnJ2ZmSlnu6zVk5OT5Sxvt7KyUs5ub2+Xs/Pz8+Vsl/PWxMREOdtljT+quqw/N2/eLGe7XDMNBoNytsvz5zDYmQIAAChQpgAAAAqUKQAAgAJlCgAAoECZAgAAKFCmAAAACpQpAACAAmUKAACgQJkCAAAoUKYAAAAKlCkAAIACZQoAAKBAmQIAAChQpgAAAArGDuuBd3Z2ytkTJ04c4CQ/GF3+vZOTkwc4ydGwsLBQzs7Pz5ezff1f7O7u9nLco6jL1/LixYvl7OrqajnbxcrKSi/H5XbT09Pl7KuvvlrOzszM9JK9du1aOXsU1721tbVy9umnny5nT58+Xc52cenSpXL22WefPcBJ6LL+rK+vl7Obm5vlbJfnfBddri+/GztTAAAABcoUAABAgTIFAABQoEwBAAAUKFMAAAAFyhQAAECBMgUAAFCgTAEAABQoUwAAAAXKFAAAQIEyBQAAUKBMAQAAFChTAAAABcoUAABAwdhhPfDk5GQ5u7GxcYCT3LmdnZ1y9saNG+XsqVOnylmOhs3NzXJ2MBgc4CSjb3l5uZy9dOnSwQ3yDqyurpazExMTBzgJfemyZl67dq2cPXv2bDl74cKFcvb8+fPl7N1qfHy8l+yVK1fK2S5rTxdzc3O9HJfbDYfDvkd4x7a3t/se4W3sTAEAABQoUwAAAAXKFAAAQIEyBQAAUKBMAQAAFChTAAAABcoUAABAgTIFAABQoEwBAAAUKFMAAAAFyhQAAECBMgUAAFCgTAEAABQoUwAAAAVjh/XA09PT5eyNGzfK2atXr/aS7WJxcbGX48JRND8/X86ur6+Xszdv3ixn5+bmytnZ2dly9sknn+zluEfV0tJSOTszM1PO7uzslLPPP/98OXvq1Kly9igaDofl7O7ubjm7ublZznaZ+fTp0+XsxMREOcvt1tbWytnx8fFydnl5uZztosuaeRjsTAEAABQoUwAAAAXKFAAAQIEyBQAAUKBMAQAAFChTAAAABcoUAABAgTIFAABQoEwBAAAUKFMAAAAFyhQAAECBMgUAAFCgTAEAABQoUwAAAAVjh/XA09PT5eyFCxfK2cXFxXL20UcfLWc3NjbKWQ7WxMREOTs7O1vOrq2tlbPr6+vl7Pz8fDl7FA0Gg3J2c3Ozl+zy8nI52+V5NzU1Vc52+V45qiYnJ8vZM2fOHOAkd+7UqVPl7OXLlw9wEqq6rHm3bt0qZ609d4/r16+Xs5cuXTrASe7c6dOny9nhcHhwgxwAO1MAAAAFyhQAAECBMgUAAFCgTAEAABQoUwAAAAXKFAAAQIEyBQAAUKBMAQAAFChTAAAABcoUAABAgTIFAABQoEwBAAAUKFMAAAAFyhQAAEBBttb6ngEAAGDk2JkCAAAoUKYAAAAKlCkAAIACZQoAAKBAmQIAAChQpgAAAAr+D/TLOpcWFnLKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x1080 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# getting the dataset \n",
    "digits = load_digits()\n",
    "\n",
    "# visualizing the images in the dataset (from image key)\n",
    "print(\" The images in the given dataset are as follows:\")\n",
    "plt.figure(figsize = (15,15))\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:6]):\n",
    "    plt.subplot(1, 6, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r)\n",
    "    plt.title('%i' % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of the original dataset is (1797, 64)\n",
      "The dimension of the training data is:  (1437, 64)\n",
      "The dimension of the test data is:  (360, 64)\n"
     ]
    }
   ],
   "source": [
    "# splitting the dataset as train and test datasets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(digits.data, digits.target, test_size = 0.20, random_state = 1)\n",
    "print('The dimension of the original dataset is', digits.data.shape)\n",
    "print(\"The dimension of the training data is: \", X_train.shape)\n",
    "print(\"The dimension of the test data is: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score computed is: \n",
      " 0.9694444444444444\n",
      "\n",
      "Confusion matrix:\n",
      "[[42  0  0  0  1  0  0  0  0  0]\n",
      " [ 0 34  0  0  0  0  0  0  1  0]\n",
      " [ 0  0 35  1  0  0  0  0  0  0]\n",
      " [ 0  0  0 39  0  0  0  0  1  1]\n",
      " [ 0  0  0  0 38  0  0  0  0  0]\n",
      " [ 0  0  0  1  0 29  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 37  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 36  0  1]\n",
      " [ 0  0  0  0  0  1  0  0 27  1]\n",
      " [ 0  0  0  0  0  1  0  0  1 32]]\n",
      "\n",
      "Classification report for logistic regression classifier \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      0.97      0.99        35\n",
      "           2       1.00      0.97      0.99        36\n",
      "           3       0.95      0.95      0.95        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.94      0.97      0.95        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.90      0.93      0.92        29\n",
      "           9       0.91      0.94      0.93        34\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       360\n",
      "   macro avg       0.97      0.97      0.97       360\n",
      "weighted avg       0.97      0.97      0.97       360\n",
      "\n",
      "\n",
      "Number of mislabeled points out of a total 360 points : 11\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression without tuning\n",
    "logreg = LogisticRegression()\n",
    "log_fit = logreg.fit(X_train,Y_train)\n",
    "\n",
    "y_pred = log_fit.predict(X_test)\n",
    "\n",
    "print(\"F1-Score computed is: \\n %s\\n\"\n",
    "      % (metrics.f1_score(Y_test, y_pred, average='micro')))\n",
    "\n",
    "print(\"Confusion matrix:\\n%s\\n\" % metrics.confusion_matrix(Y_test, y_pred))\n",
    "\n",
    "print(\"Classification report for logistic regression classifier \\n%s\\n\"\n",
    "      % (metrics.classification_report(Y_test, y_pred)))\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0],(Y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal C value is:  0.015510204081632653\n"
     ]
    }
   ],
   "source": [
    "# using GridSearchCV to tune the logistic regression Model\n",
    "rng = np.linspace(0.01,0.1,50)\n",
    "parameters = {'C': rng}\n",
    "reg = LogisticRegression()\n",
    "clf = GridSearchCV(reg, parameters, cv = 5)\n",
    "clf.fit(X_train, Y_train)\n",
    "print(\"The optimal C value is: \", clf.best_params_['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score computed is: \n",
      " 0.975\n",
      "\n",
      "Confusion matrix:\n",
      "[[42  0  0  0  1  0  0  0  0  0]\n",
      " [ 0 33  0  0  1  0  1  0  0  0]\n",
      " [ 0  0 36  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 40  0  1  0  0  0  0]\n",
      " [ 0  0  0  0 38  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 30  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 37  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 36  0  1]\n",
      " [ 0  0  0  0  0  1  0  0 27  1]\n",
      " [ 0  0  0  0  0  1  0  0  1 32]]\n",
      "\n",
      "Classification report for logistic regression classifier \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      0.94      0.97        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.98      0.99        41\n",
      "           4       0.95      1.00      0.97        38\n",
      "           5       0.91      1.00      0.95        30\n",
      "           6       0.97      1.00      0.99        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.96      0.93      0.95        29\n",
      "           9       0.94      0.94      0.94        34\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       360\n",
      "   macro avg       0.97      0.97      0.97       360\n",
      "weighted avg       0.98      0.97      0.98       360\n",
      "\n",
      "\n",
      "Number of mislabeled points out of a total 360 points : 9\n"
     ]
    }
   ],
   "source": [
    "# Building the Logistic Regression Model with the optimal parameters\n",
    "logreg = LogisticRegression(C = clf.best_params_['C'])\n",
    "log_fit = logreg.fit(X_train,Y_train)\n",
    "\n",
    "y_pred = log_fit.predict(X_test)\n",
    "\n",
    "print(\"F1-Score computed is: \\n %s\\n\"\n",
    "      % (metrics.f1_score(Y_test, y_pred, average='micro')))\n",
    "\n",
    "print(\"Confusion matrix:\\n%s\\n\" % metrics.confusion_matrix(Y_test, y_pred))\n",
    "\n",
    "print(\"Classification report for logistic regression classifier \\n%s\\n\"\n",
    "      % (metrics.classification_report(Y_test, y_pred)))\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0],(Y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The images in the test dataset and their predictions are as follows:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing test dataset along with the predictions of Logistic Regression Classifier\n",
    "test_images = [X_test[i].reshape(8,8) for i in range(len(X_test))]\n",
    "images_and_predictions = list(zip(test_images, y_pred))\n",
    "print(\" The images in the test dataset and their predictions are as follows:\")\n",
    "plt.figure(figsize = (18,18))\n",
    "for index, (image, prediction) in enumerate(images_and_predictions[:10]):\n",
    "    plt.subplot(1, 10, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r)\n",
    "    plt.title('%i' % prediction)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "- Logistic Regression performs fairly well on the given dataset with a tuned (after tuning the regularization parameter (C))  f1 score of 0.975 with a miss classification error of 9 out of 360 images in the test dataset.\n",
    "- The classification report is also printed to get the info about all the classes present (0 to 9) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES CLASSIFICATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 360 points : 53\n",
      "\n",
      "F1-Score computed using Gaussian prior is: \n",
      " 0.8527777777777777\n",
      "\n",
      "Confusion matrix:\n",
      "[[41  0  0  0  1  0  0  1  0  0]\n",
      " [ 0 29  0  0  0  0  0  0  6  0]\n",
      " [ 0  3 23  1  0  0  0  0  9  0]\n",
      " [ 0  1  1 32  0  2  0  2  3  0]\n",
      " [ 1  0  0  0 35  0  1  0  1  0]\n",
      " [ 0  1  0  0  0 25  0  2  2  0]\n",
      " [ 0  0  0  0  0  0 37  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 37  0  0]\n",
      " [ 0  0  0  1  0  0  0  0 28  0]\n",
      " [ 0  1  0  1  2  1  0  5  4 20]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using gaussian prior\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, Y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\\n\" % (X_test.shape[0],(Y_test != y_pred).sum()))\n",
    "\n",
    "print(\"F1-Score computed using Gaussian prior is: \\n %s\\n\"\n",
    "      % (metrics.f1_score(Y_test, y_pred, average='micro')))\n",
    "\n",
    "print(\"Confusion matrix:\\n%s\\n\" % metrics.confusion_matrix(Y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 360 points : 33\n",
      "\n",
      "F1-Score computed using Multinomial Naive Bayes is: \n",
      " 0.9083333333333333\n",
      "\n",
      "Confusion matrix:\n",
      "[[41  0  0  0  2  0  0  0  0  0]\n",
      " [ 0 24  5  0  0  0  1  0  3  2]\n",
      " [ 0  1 31  0  0  0  0  0  2  2]\n",
      " [ 0  0  0 34  0  0  0  1  3  3]\n",
      " [ 1  0  0  0 36  0  0  1  0  0]\n",
      " [ 0  0  0  0  0 29  0  0  0  1]\n",
      " [ 0  0  0  0  0  0 37  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 36  1  0]\n",
      " [ 0  0  0  0  0  0  0  0 28  1]\n",
      " [ 0  0  0  0  0  0  0  3  0 31]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using Multinomial Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "multiNB = MultinomialNB()\n",
    "y_pred = multiNB.fit(X_train, Y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\\n\" % (X_test.shape[0],(Y_test != y_pred).sum()))\n",
    "\n",
    "print(\"F1-Score computed using Multinomial Naive Bayes is: \\n %s\\n\"\n",
    "      % (metrics.f1_score(Y_test, y_pred, average='micro')))\n",
    "\n",
    "print(\"Confusion matrix:\\n%s\\n\" % metrics.confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 360 points : 47\n",
      "\n",
      "F1-Score computed using Bernoulli prior is: \n",
      " 0.8694444444444445\n",
      "\n",
      "Confusion matrix:\n",
      "[[42  1  0  0  0  0  0  0  0  0]\n",
      " [ 0 20  7  0  0  0  0  0  6  2]\n",
      " [ 0  1 31  2  0  0  0  1  1  0]\n",
      " [ 0  0  1 33  0  0  0  1  2  4]\n",
      " [ 0  0  0  0 37  0  0  1  0  0]\n",
      " [ 0  1  0  0  0 27  0  0  1  1]\n",
      " [ 0  0  0  0  0  0 37  0  0  0]\n",
      " [ 0  0  0  0  1  0  0 35  1  0]\n",
      " [ 0  0  0  0  0  1  0  0 25  3]\n",
      " [ 0  2  0  2  0  1  0  3  0 26]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using Bernoulli prior\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "y_pred = bnb.fit(X_train, Y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\\n\" % (X_test.shape[0],(Y_test != y_pred).sum()))\n",
    "\n",
    "print(\"F1-Score computed using Bernoulli prior is: \\n %s\\n\"\n",
    "      % (metrics.f1_score(Y_test, y_pred, average='micro')))\n",
    "\n",
    "print(\"Confusion matrix:\\n%s\\n\" % metrics.confusion_matrix(Y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oli7OP9XN4ZF"
   },
   "source": [
    "### Observation\n",
    "- The MultinomialNB gives the best f1 score of 0.90833 as against GaussianNB(0.85277) and BernoulliNB(0.86944). So, Multinomial Prior is the best prior.\n",
    "- The accuracies are reported in terms of F1 score and Confusion Matrix in all the three cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aiq6KaGLOAba"
   },
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab5_DataAnalytics.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
